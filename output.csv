Video Title (from TOC),Video Filename,Video ID,Quiz,Exam,CEU,Qustn Type,Question,Correct,Incorrect 1,Incorrect 2,Incorrect 3,Correct Explanation,Incorrect 1 Explanation,Incorrect 2 Explanation,Incorrect 3 Explanation,Subdomain,Difficulty Level,Is Practice Exam,Good/Bug/Needs Improvement,Actionable Feedback,Matches Learning Objective
00_03 Overview of CICD,,,,,,,What does the acronym CI/CD stand for in the context of software development?,"Continuous Integration, Continuous Delivery, Continuous Deployment","Continuous Integration, Continuous Development, Continuous Deployment","Continuous Improvement, Continuous Delivery, Continuous Deployment","Continuous Integration, Continuous Deployment, Continuous Debugging","CI/CD stands for Continuous Integration, Continuous Delivery, and Continuous Deployment, which are phases of software development.",Continuous Development is not part of the CI/CD acronym; Continuous Delivery is the correct term.,Continuous Improvement is not part of the CI/CD acronym; Continuous Integration is the correct term.,Continuous Debugging is not part of the CI/CD acronym; Continuous Delivery is the correct term.,,,,,,
00_03 Overview of CICD,,,,,,,What is the main goal of Continuous Integration (CI)?,To identify and resolve problems quickly and early in the development process.,To automate the deployment of software into production.,To package software and prepare it for release.,To integrate multiple features and deliver them simultaneously.,"The main goal of Continuous Integration (CI) is to allow teams to identify and resolve problems quickly and early in the development process by regularly committing code to a shared repository, testing it, and checking for errors or requirements.","Automating the deployment of software into production is the goal of Continuous Deployment, not Continuous Integration.","Packaging software and preparing it for release is the goal of Continuous Delivery, not Continuous Integration.","Integrating multiple features and delivering them simultaneously is related to system-level tests in Continuous Delivery, not the main goal of Continuous Integration.",,,,,,
00_03 Overview of CICD,,,,,,,How does Continuous Deployment differ from Continuous Delivery in terms of automation?,Continuous Deployment involves fully automated deployment to production environments without human interaction.,Continuous Deployment requires manual approval before deploying to production.,Continuous Deployment focuses on integrating code changes into a shared repository.,Continuous Deployment packages the software and prepares it for release.,"Continuous Deployment involves fully automating the deployment process to production environments without human interaction, ensuring rapid and reliable software releases.",Continuous Deployment does not require manual approval; it is fully automated.,"Integrating code changes into a shared repository is the focus of Continuous Integration, not Continuous Deployment.","Packaging the software and preparing it for release is the focus of Continuous Delivery, not Continuous Deployment.",,,,,,
00_04 Understanding Pipeline Limits,,,,,,,What is the purpose of Large File Storage (LFS) in Bitbucket?,To store binary files that are larger than typical text files used to store code.,To store small text files used for documentation.,To store only image files.,To store only video files.,Large File Storage (LFS) in Bitbucket is intended for binary files that are larger than the typical text files used to store code.,LFS is not intended for small text files; it is meant for larger binary files.,"While LFS can store image files, it is not limited to image files only; it is for any larger binary files.","LFS can store video files, but it is not exclusive to video files; it is for larger binary files in general.",,,,,,
00_04 Understanding Pipeline Limits,,,,,,,How do the limitations of the Bitbucket free tier impact the scalability of a team's CI/CD processes?,The limitations on storage and build minutes can restrict a team's ability to scale their CI/CD processes as their projects and team size grow.,The limitations only affect the number of repositories a team can create.,The limitations prevent any form of automated testing.,The limitations do not impact the CI/CD process in any significant way.,"The limitations on storage and build minutes in the Bitbucket free tier can restrict a team's ability to scale their CI/CD processes because they may quickly run out of the allocated resources as their projects and team size grow, necessitating a move to a paid plan for more resources.",The limitations on storage and build minutes do not affect the number of repositories; Bitbucket offers unlimited repositories even in the free tier.,"The limitations do not prevent automated testing entirely, but the limited build minutes can restrict the amount of automated testing that can be performed.","The limitations do impact the CI/CD process, particularly in terms of scalability and resource availability.",,,,,,
01_02 Bitbucket Pipelines Configuration,,,,,,,Where should the Bitbucket Pipelines configuration file be placed in a repository?,At the root of the repository,"In a folder named ""pipelines""","In the ""src"" directory","In a hidden "".config"" directory","The Bitbucket Pipelines configuration file, named `bitbucket-pipelines.yml`, should be placed at the root of the repository.","The configuration file is not placed in a folder named ""pipelines""; it should be placed at the root of the repository.","The configuration file is not placed in the ""src"" directory; it should be placed at the root of the repository.","The configuration file is not placed in a hidden "".config"" directory; it should be placed at the root of the repository.",,,,,,
01_02 Bitbucket Pipelines Configuration,,,,,,,What is the purpose of the `pipelines` keyword in the `bitbucket-pipelines.yml` file?,It identifies the start of specific pipeline configurations.,It specifies the Docker image to use for the pipeline.,It defines the triggers for the pipeline.,It lists the steps within the pipeline.,"The `pipelines` keyword is used to identify the start of specific pipeline configurations in the `bitbucket-pipelines.yml` file, allowing multiple pipelines to be configured in a single file.","Specifying the Docker image is done within the pipeline configuration, not with the `pipelines` keyword.",Triggers are defined separately within the pipeline configuration and are not associated with the `pipelines` keyword.,"Steps are listed within the pipeline configuration, but the `pipelines` keyword identifies the start of the configuration.",,,,,,
03_01 Default Images,,,,,,,What are the implications of not specifying a Docker image in your Bitbucket pipeline configuration?,The pipeline will use a default image provided by Bitbucket.,The pipeline will fail to run.,The pipeline will run without any Docker image.,The pipeline will use an image specified in the previous configuration.,"If no Docker image is specified in the Bitbucket pipeline configuration, a default image provided by Bitbucket will be used for the execution environment.",The pipeline will not fail to run; it will use the default image provided by Bitbucket.,The pipeline requires an execution environment and will use a default image if none is specified.,The pipeline does not inherit images from previous configurations; it uses a default image if none is specified in the current configuration.,,,,,,
01_03 Configuring Pipeline Stages,,,,,,,What keyword is used to group multiple steps together in a Bitbucket pipeline configuration?,stage,group,section,,"The `stage` keyword is used in Bitbucket pipeline configuration to group multiple steps together, allowing for better organization and sequential execution within that stage.",The `group` keyword is not used in Bitbucket pipeline configuration; `stage` is the correct keyword for grouping steps.,The `section` keyword is not used to group steps; `stage` is the correct keyword for this purpose.,,,,,,,
01_03 Configuring Pipeline Stages,,,,,,,You have a pipeline with three integration steps running in parallel and two deployment steps. How can you group the deployment steps together?,Use the `stage` keyword to group the deployment steps together.,Use the `parallel` keyword to group the deployment steps together.,Use the `steps` keyword to group the deployment steps together.,Use the `trigger` keyword to group the deployment steps together.,"To group deployment steps together, you should use the `stage` keyword in the Bitbucket pipeline configuration, which allows you to organize steps into stages for better management and execution control.","The `parallel` keyword is used to run steps concurrently, not to group them into stages.","The `steps` keyword defines individual steps within a stage or pipeline, not for grouping steps into stages.","The `trigger` keyword is used to define conditions for starting pipelines, not for grouping steps into stages.",,,,,,
01_03 Configuring Pipeline Stages,,,,,,,What are the implications of having a stage name that is not unique within a Bitbucket pipeline configuration?,It will cause errors in the YAML formatting and prevent the pipeline from running correctly.,It will allow multiple stages to run in parallel.,It will merge the steps of both stages into one.,It will have no impact on the pipeline's execution.,"A stage name must be unique within a Bitbucket pipeline configuration. If it is not unique, it will cause errors in the YAML formatting and prevent the pipeline from running correctly.",Having a non-unique stage name will not allow multiple stages to run in parallel; it will cause errors.,Having a non-unique stage name will not merge the steps of both stages; it will cause errors.,Having a non-unique stage name does impact the pipeline's execution by causing errors.,,,,,,
01_04 Configure Pipeline Triggers,,,,,,,Why is the `default` trigger commonly referred to as a commit trigger?,Because it runs the associated pipeline on every commit to the repository.,Because it only runs on the default branch.,Because it requires manual intervention to start.,Because it only triggers pipelines for pull requests.,"The `default` trigger is commonly referred to as a commit trigger because it starts the associated pipeline on every commit to the repository, making it useful for frequent integration steps like compiling software or running tests.","The `default` trigger can run on any branch, not just the default branch.",The `default` trigger does not require manual intervention; it runs automatically on commits.,The `default` trigger is not limited to pull requests; it runs on commits.,,,,,,
01_04 Configure Pipeline Triggers,,,,,,,"How can you configure a pipeline to run for all branches with names that start with ""feature/"" using a wildcard pattern?",Use the pattern `feature/*` under the `branches` keyword.,Use the pattern `feature-branches/*` under the `branches` keyword.,Use the pattern `*feature*` under the `branches` keyword.,Use the pattern `feature-**` under the `branches` keyword.,"To configure a pipeline to run for all feature branches, you should use the pattern `feature/*` under the `branches` keyword. This pattern matches any branch that starts with ""feature/"".",The pattern `feature-branches/*` is not a common naming convention and may not match the intended branches.,The pattern `*feature*` is not a correct usage of wildcards in Bitbucket Pipelines and may not work as expected.,The pattern `feature-**` is incorrect and may not match the intended branches.,,,,,,
01_05 Use YAML Anchors,,,,,,,What does DRY stand for in the context of software development?,Don’t Repeat Yourself,Do Repeat Yourself,Data Replication Yardstick,Dynamic Resource Yield,"In software development, DRY stands for ""Don’t Repeat Yourself,"" which emphasizes the importance of reducing duplication in code.","DRY stands for ""Don’t Repeat Yourself,"" not ""Do Repeat Yourself.""","""Data Replication Yardstick"" is not related to the DRY principle in software development.","""Dynamic Resource Yield"" is not related to the DRY principle in software development.",,,,,,
01_05 Use YAML Anchors,,,,,,,How do YAML anchors help in maintaining pipeline configurations?,"They allow you to define reusable steps that can be referenced multiple times, reducing the need for code duplication.",They enable automatic error checking in the pipeline configuration.,They help in defining environment variables for the pipeline.,They provide a way to encrypt sensitive information in the pipeline configuration.,"YAML anchors help in maintaining pipeline configurations by allowing the definition of reusable steps that can be referenced multiple times, thus reducing the need for repeating the same code in multiple places.","While useful, YAML anchors do not provide automatic error checking in the pipeline configuration.",Defining environment variables is not the primary purpose of YAML anchors; they are used for reusing steps.,YAML anchors do not provide encryption for sensitive information; they are used for creating reusable steps.,,,,,,
01_05 Use YAML Anchors,,,,,,,How would you reference a YAML anchor named `build_and_test` in a pipeline step?,Use `- step: *build_and_test`,Use `- step: &build_and_test`,Use `- step: build_and_test`,Use `- step: ${build_and_test}`,"To reference a YAML anchor named `build_and_test`, you use `- step: *build_and_test`. The asterisk followed by the anchor's alias denotes the reference.","The ampersand is used when defining an anchor, not when referencing it.",Simply using the name without an asterisk does not correctly reference a YAML anchor.,The `${}` syntax is not used for referencing YAML anchors.,,,,,,
01_05 Use YAML Anchors,,,,,,,What is the effect of using the `<<:` prefix when referencing a YAML anchor?,It allows overriding top-level values in the referenced anchor.,It prevents the anchor from being reused.,It encrypts the values in the anchor.,It duplicates the anchor in the pipeline configuration.,"The `<<:` prefix is used when referencing a YAML anchor to allow overriding top-level values in the referenced anchor, providing flexibility in customizing steps.",The `<<:` prefix does not prevent the anchor from being reused; it allows for customization.,The `<<:` prefix does not encrypt values; it is used for overriding values.,The `<<:` prefix does not duplicate the anchor; it allows for value overrides in the reference.,,,,,,
02_01 Use Variables and Secrets,,,,,,,Which of the following is a common practice for naming variables in Bitbucket pipeline configurations?,Using all capital letters for variable names,Using all lowercase letters for variable names,Using numbers at the beginning of variable names,Using special characters in variable names,Using all capital letters for variable names is a common practice in Bitbucket pipeline configurations to make variables easier to find.,Using all lowercase letters is not the common practice for naming variables in Bitbucket pipeline configurations.,Using numbers at the beginning of variable names is not a recommended practice and can cause issues.,Using special characters in variable names is not a common practice and can lead to errors.,,,,,,
02_01 Use Variables and Secrets,,,,,,,Why are secret values in Bitbucket pipeline configurations obscured in pipeline logs and output?,To protect sensitive details like API keys and passwords from being exposed.,To improve the performance of the pipeline.,To make the logs easier to read.,To reduce the size of the log files.,Secret values in Bitbucket pipeline configurations are obscured in pipeline logs and output to protect sensitive information such as API keys and passwords from being exposed.,Obscuring secret values is not related to improving pipeline performance.,"The primary reason for obscuring secret values is to protect sensitive information, not to make the logs easier to read.",Obscuring secret values does not significantly reduce the size of log files; it is done to protect sensitive information.,,,,,,
02_01 Use Variables and Secrets,,,,,,,How would you reference a predefined variable named `BITBUCKET_BUILD_NUMBER` in a pipeline script?,`$BITBUCKET_BUILD_NUMBER`,`BITBUCKET_BUILD_NUMBER`,`#BITBUCKET_BUILD_NUMBER`,`&BITBUCKET_BUILD_NUMBER`,Predefined variables like `BITBUCKET_BUILD_NUMBER` can be referenced in a pipeline script using `$BITBUCKET_BUILD_NUMBER` or `${BITBUCKET_BUILD_NUMBER}`.,Simply using the variable name without a `$` or `${}` will not reference the variable correctly.,The `#` character is not used for referencing variables in Bitbucket pipeline scripts.,The `&` character is not used for referencing variables in Bitbucket pipeline scripts.,,,,,,
02_02 Deployment Variables,,,,,,,Why are deployment variables useful in Bitbucket Pipelines?,They allow the same steps to be reused for different environments by using different variable values.,They automatically encrypt all pipeline variables.,They improve the performance of the pipeline runs.,They provide version control for the pipeline configuration.,"Deployment variables are useful because they enable the same pipeline steps to be reused for different environments by using different variable values, which helps in maintaining a DRY configuration.",Deployment variables do not automatically encrypt all pipeline variables; they are used for specifying environment-specific values.,Deployment variables are not primarily used to improve performance but to provide flexibility and maintainability in the pipeline configuration.,Deployment variables do not provide version control for the pipeline configuration; they manage environment-specific values.,,,,,,
02_02 Deployment Variables,,,,,,,"How would you add a new environment named ""Load Testing"" under the staging group in Bitbucket?","Select “Add environment” under the staging group and enter ""Load Testing"" as the name.","Add the environment in the workspace settings and name it ""Load Testing.""","Modify the pipeline configuration file to include ""Load Testing.""","Use the default environment and rename it to ""Load Testing.""","To add a new environment named ""Load Testing"" under the staging group, you select “Add environment” under the staging group in the repository settings and enter ""Load Testing"" as the name.","Adding environments should be done under the specific group in the repository settings, not in the workspace settings.","Adding an environment is done through the Bitbucket web interface, not by modifying the pipeline configuration file directly.",Using the default environment and renaming it is not the correct approach for adding a new environment.,,,,,,
02_03 Artifacts,,,,,,,What is an artifact in the context of Bitbucket Pipelines?,A file written to disk by a pipeline step,A variable used in the pipeline configuration,A log message generated during the pipeline run,A predefined environment variable,"In Bitbucket Pipelines, an artifact is a file that is written to disk by a pipeline step, such as a compiled executable, an archive, or a text file with log messages.",A variable used in the pipeline configuration is not considered an artifact.,"While a log message may be included in an artifact, it is not an artifact itself.",Predefined environment variables are not artifacts; artifacts are files created during pipeline execution.,,,,,,
02_03 Artifacts,,,,,,,What is the benefit of sharing artifacts between steps in a pipeline?,"It minimizes pipeline run times, saving build minutes and making pipelines more manageable.",It encrypts sensitive data in the pipeline.,It automatically generates documentation for the pipeline.,It provides version control for the pipeline configuration.,"Sharing artifacts between steps in a pipeline minimizes pipeline run times, saving build minutes and making pipelines more manageable by avoiding the need to recreate the same files in multiple steps.",Sharing artifacts does not encrypt sensitive data; it is used to share files between steps.,Sharing artifacts does not generate documentation; it optimizes the reuse of files between steps.,Sharing artifacts does not provide version control for the pipeline configuration; it helps in managing files generated during the pipeline run.,,,,,,
02_03 Artifacts,,,,,,,How would you specify that all zip files created by a step should be treated as artifacts?,Use `artifacts: - '*.zip'`,"Use `artifacts: - ""*.zip""`",Use `artifacts: - '*.zip*'`,Use `artifacts: - zip/*`,"To specify that all zip files created by a step should be treated as artifacts, you use `artifacts: - '*.zip'`. The single quotes around the wildcard pattern are required.",Double quotes should not be used for wildcard patterns in the artifacts list.,The pattern `*.zip*` is not correct for capturing zip files.,"The pattern `zip/*` would capture files in a ""zip"" directory, not all zip files created by the step.",,,,,,
02_04 Packages,,,,,,,How long are artifacts available in Bitbucket Pipelines by default?,14 days,7 days,30 days,Indefinitely,"By default, artifacts in Bitbucket Pipelines are available for 14 days after they are created.",Artifacts are not available for just 7 days; the default duration is 14 days.,Artifacts are not available for 30 days; the default duration is 14 days.,Artifacts are not available indefinitely by default; they are available for 14 days.,,,,,,
02_04 Packages,,,,,,,Why are packages a better solution than artifacts for long-term use?,Packages are available indefinitely and can be accessed from a consistent location.,Packages automatically encrypt all stored files.,Packages are limited to a specific pipeline run.,Packages are only available for private repositories.,"Packages are a better solution for long-term use because they are available indefinitely and can be accessed from a consistent location, unlike artifacts which are only available for 14 days.",Packages do not automatically encrypt all stored files; their advantage lies in accessibility and longevity.,Packages are not limited to a specific pipeline run; they can be accessed from a consistent location.,"Packages are available for both public and private repositories, not just private ones.",,,,,,
02_04 Packages,,,,,,,How would you use an access token in a Bitbucket pipeline to upload a file as a package?,Save the token as a repository variable and pass it to the `bitbucket-upload-file` pipe.,Include the token directly in the pipeline script.,Use the token in the artifact configuration.,Store the token in a YAML anchor.,"To use an access token in a Bitbucket pipeline to upload a file as a package, save the token as a repository variable and use the `bitbucket-upload-file` pipe in the pipeline configuration.",Including the token directly in the pipeline script is not secure; it should be saved as a repository variable.,The token is not used in the artifact configuration; it is used in the `bitbucket-upload-file` pipe.,Storing the token in a YAML anchor is not the correct approach; it should be saved as a repository variable.,,,,,,
02_04 Packages,,,,,,,What could be the impact of not having an access token when attempting to upload a package from a pipeline?,The pipeline will fail to upload the package due to lack of necessary permissions.,The pipeline will succeed but the package will be corrupted.,The pipeline will automatically generate an access token.,The pipeline will skip the upload step without any errors.,"Without an access token, the pipeline will fail to upload the package because it does not have the necessary permissions to perform the upload.",The pipeline will not succeed in uploading without an access token; it will fail due to permission issues.,The pipeline does not automatically generate an access token; it must be provided.,The pipeline will not skip the upload step silently; it will fail and generate an error due to lack of permissions.,,,,,,
03_01 Default Images,,,,,,,What operating system is the default image used by Bitbucket Pipelines based on?,Ubuntu Linux,CentOS,Fedora,Debian,"The default image used by Bitbucket Pipelines is based on the Ubuntu Linux operating system, providing a consistent environment with standard Linux commands and additional tools.",CentOS is not the operating system for the default image; it is based on Ubuntu Linux.,Fedora is not the operating system for the default image; it is based on Ubuntu Linux.,"While Debian is similar to Ubuntu, the default image is specifically based on Ubuntu Linux.",,,,,,
03_01 Default Images,,,,,,,Why is it beneficial to run pipeline steps in Docker containers?,"Containers provide consistent, reproducible, and isolated build environments.",Containers increase the speed of pipeline execution.,Containers allow for unlimited storage during pipeline runs.,Containers automatically generate pipeline documentation.,"Running pipeline steps in Docker containers ensures that each step starts from a known good state, providing consistent, reproducible, and isolated build environments, which is crucial for reliable builds.","While containers may help with reliability, their primary benefit is not necessarily increasing the speed of execution.",Containers do not provide unlimited storage; they offer isolated environments with defined resources.,Containers do not generate documentation; they provide isolated build environments.,,,,,,
03_01 Default Images,,,,,,,How would you specify a Docker image for an individual step in a Bitbucket pipeline?,Use the `image` keyword followed by the image name within the step definition.,Use the `container` keyword followed by the image name within the step definition.,Use the `docker` keyword followed by the image name within the step definition.,Use the `os` keyword followed by the image name within the step definition.,"To specify a Docker image for an individual step in a Bitbucket pipeline, use the `image` keyword followed by the image name within the step definition.",The `container` keyword is not used to specify Docker images in Bitbucket pipelines.,The `docker` keyword is not used to specify Docker images in Bitbucket pipelines.,The `os` keyword is not used to specify Docker images in Bitbucket pipelines.,,,,,,
03_02 Public and Custom Images,,,,,,,Why might a developer choose to use a preconfigured container image in a Bitbucket pipeline?,To save build time and avoid reinstalling tools in each pipeline step.,To ensure the container runs on the host machine's operating system.,To encrypt the pipeline logs automatically.,To bypass the need for Docker in the pipeline.,"Using a preconfigured container image in a Bitbucket pipeline saves build time because the necessary tools are already installed, avoiding the need to reinstall tools in each pipeline step and thus optimizing the overall execution time.",The container does not run on the host machine's operating system; it runs in an isolated Docker environment.,Using a preconfigured container image does not automatically encrypt pipeline logs; it ensures that required tools are readily available.,Using Docker images is fundamental to how Bitbucket Pipelines operate; it does not bypass the need for Docker.,,,,,,
03_03 Services,,,,,,,What is a service in the context of Bitbucket Pipelines?,An additional container that runs alongside the pipeline steps to provide connections to other applications.,A built-in Bitbucket tool for code analysis.,A cloud storage solution for pipeline artifacts.,A method for parallelizing pipeline steps.,"A service in Bitbucket Pipelines is an additional container that runs alongside the pipeline steps to provide connections to other applications such as databases, caching services, or request queues.",Services are not built-in tools for code analysis; they are additional containers for auxiliary applications.,Services are not cloud storage solutions for pipeline artifacts; they are additional containers for auxiliary applications.,Services are not methods for parallelizing pipeline steps; they are additional containers for auxiliary applications.,,,,,,
03_03 Services,,,,,,,Why is it important to consider memory allocation when using services in Bitbucket Pipelines?,"Because the memory is split between the steps and the services, which can affect memory-intensive tasks.",Because services cannot run in parallel with pipeline steps.,Because services require dedicated network connections.,Because services increase the pipeline execution time significantly.,It is important to consider memory allocation when using services in Bitbucket Pipelines because the memory is split between the steps and the services. This split can affect the performance of memory-intensive tasks.,"Services can run in parallel with pipeline steps, but memory allocation must be considered.","While services may require network connections, the primary concern here is memory allocation.",Services do not necessarily increase the pipeline execution time significantly; the concern is about memory allocation.,,,,,,
03_03 Services,,,,,,,What could be the impact of not adding wait time for services in a Bitbucket pipeline step?,Tests may fail because they try to access a service before it has started.,The pipeline will automatically adjust and wait for the service.,The service will not start at all.,The pipeline execution will be faster and more efficient.,"If wait time is not added for services in a Bitbucket pipeline step, tests may fail because they might try to access a service before it has fully started. It is important to ensure services are ready before running tests that depend on them.",The pipeline does not automatically wait for services; wait times need to be manually added.,"The service will start, but without wait time, tests may fail if they access the service too soon.","Not adding wait time may lead to test failures, which does not make the pipeline execution more efficient.",,,,,,
